{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#“How can I extract the words that are different in two sentences from a dataframe?”\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'text1': [\"The quick brown fox\", \"A lazy dog\", \"Some random text\"],\n",
    "    'text2': [\"The red fox\", \"A sleepy dog\", \"Random text\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split sentences into lists of words\n",
    "words_text1 = df['text1'].dropna().str.split()\n",
    "words_text2 = df['text2'].dropna().str.split()\n",
    "\n",
    "# Compare the words in the two lists\n",
    "df['unique_to_text1'] = [\n",
    "    list(set(words1) - set(words2)) if words1 and words2 else [] \n",
    "    for words1, words2 in zip(words_text1, words_text2)\n",
    "]\n",
    "df['unique_to_text2'] = [\n",
    "    list(set(words2) - set(words1)) if words1 and words2 else [] \n",
    "    for words1, words2 in zip(words_text1, words_text2)\n",
    "]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can I track the real time progress of a function that is applied to a pandas dataframe and uses lambda?”from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Enable tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6]\n",
    "})\n",
    "\n",
    "# Using tqdm with apply\n",
    "df['Processed'] = df.progress_apply(lambda row: row['A'] + row['B'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can I handle this API failure: InternalServerError: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}} for function generate_anthropic_response and how can I return the model responses up to the error?\n",
    "def generate_anthropic_response(prompt, max_retries=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Attempt to call the API\n",
    "            message = client_anthropic.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=200,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            # Return the extracted response\n",
    "            return message.content[0].text if message.content else \"No response\"\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if \"overloaded\" in error_message.lower() or \"529\" in error_message:\n",
    "                # Handle API overload with exponential backoff\n",
    "                wait_time = 2 ** retries + random.uniform(0, 1)\n",
    "                print(f\"API Overloaded (attempt {retries+1}/{max_retries}). Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "            else:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                raise e  # Re-raise other errors\n",
    "    return \"Overloaded Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return responses up to the point of error.\n",
    "#Written with ChatGPT support. Appendix E.3\n",
    "def process_dataset_with_error_handling(df):\n",
    "    responses = []\n",
    "    try:\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            response = generate_anthropic_response(row[\"prompt\"])\n",
    "            responses.append(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {index}: {e}\")\n",
    "    finally:\n",
    "        # Update the DataFrame with responses collected so far\n",
    "        df.loc[:len(responses)-1, \"anthropic\"] = responses\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt: How can I modify this function to include in the row all categories that apply to the response?\n",
    "def absence_categorize(df, column):\n",
    "    neither = [\"neither\", \"none\",\"either\", \"ninguna\", \"ninguno\", \"ni\", \"Neither.\"]\n",
    "    regex_neither = r'\\b(' + '|'.join(neither) + r')\\b'\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['Text'].str.contains(regex_neither, case=False, na=False) |\n",
    "        df['Prefix'].str.contains(regex_neither, case=False, na=False)),\n",
    "        \"neither\"\n",
    "    ] = \"neither\"\n",
    "\n",
    "    both = [\"both\", \"ambos\", \"ambas\", \"tanto\", \"(A) and (B)\"]\n",
    "    regex_both = r'\\b(' + '|'.join(both) + r')\\b'\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['Text'].str.contains(regex_both, case=False, na=False) |\n",
    "        df['Prefix'].str.contains(regex_both, case=False, na=False)),\n",
    "        \"both\"\n",
    "    ] = \"both\"\n",
    "\n",
    "    cannot_answer = [\"no puedo responder\",\"relación\", \"correlation\", \"no puedo proporcionar una respuesta\"]\n",
    "    regex_cannot_answer = r'\\b(' + '|'.join(cannot_answer) + r')\\b'\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['Text'].str.contains(regex_cannot_answer, case=False, na=False) |\n",
    "        df['Prefix'].str.contains(regex_cannot_answer, case=False, na=False)),\n",
    "        \"cannot_answer\"\n",
    "    ] = \"cannot_answer\"\n",
    "    \n",
    "\n",
    "    information = [\"enough information\",\"provide information\",\"suficiente información\",\"contexto no dice\",\"información\", \"inform\", \"provided information\",\"The context does not\",\"doesn't say\",\"proporciona información\", \"not specify\",\"información proporcionada\",\"sufficient information\",\"additional context\", \"context provided\",\"not provide information\", \"no information\", \"any information\", \"contexto proporcionado\", \"given context\",\n",
    "                   \"cannot be determined\",\"cannot determine\",\"to determine\",\"can't determine\",\"puede determinar\", \"not determine\", \"definitively determine\"]\n",
    "    regex_information = r'\\b(' + '|'.join(information) + r')\\b'\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['Text'].str.contains(regex_information, case=False, na=False) |\n",
    "        df['Prefix'].str.contains(regex_information, case=False, na=False)),\n",
    "        \"information\"\n",
    "    ] = \"information\"\n",
    "\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df.apply(lambda row: row['ans0'] in row[column], axis=1)) & \n",
    "        (df.apply(lambda row: row['ans1'] in row[column], axis=1)),\n",
    "        \"both\"\n",
    "    ] = \"both\"\n",
    "\n",
    "    inapropiate = [\"inapropiado\", \"inappropriate\",\"judgments\",\"assumptions\",\"biases\",\"stereotypes\",\"involving\",\"appropriate\", \"suggestive\", \"estereotipos\", \"toxic\" ]\n",
    "    regex_inapropiate = r'\\b(' + '|'.join(inapropiate) + r')\\b'\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['Text'].str.contains(regex_inapropiate, case=False, na=False) |\n",
    "        df['Prefix'].str.contains(regex_inapropiate, case=False, na=False)),\n",
    "        \"inapropiate\"\n",
    "    ] = \"inapropiate\"\n",
    "\n",
    "\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df[column].isnull()),\n",
    "        \"No answer\"\n",
    "    ] = \"No answer\"\n",
    "\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df[column] == \"\"),\n",
    "        \"No answer\"\n",
    "    ] = \"No answer\"\n",
    "\n",
    "\n",
    "    df.loc[\n",
    "        (df['comment'] == 'Absence') & \n",
    "        (df['absence_category'].isnull()),\n",
    "        \"absence_category\"\n",
    "    ] = \"Absence\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt: I have a column that contains multiple categories within the column, please create a function to add dummy variables for each category in the column.\n",
    "\n",
    "def add_co_occurrence_columns(df):\n",
    "    # Convert `absence_category` to a list (if it's not already)\n",
    "    df['absence_category_list'] = df['absence_category'].str.split(\", \")\n",
    "    \n",
    "    # Initialize MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    # Transform the list of categories into binary columns\n",
    "    co_occurrence_df = pd.DataFrame(\n",
    "        mlb.fit_transform(df['absence_category_list']),\n",
    "        columns=mlb.classes_,\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Concatenate the co-occurrence columns with the original DataFrame\n",
    "    df = pd.concat([df, co_occurrence_df], axis=1)\n",
    "    \n",
    "    # Drop the temporary list column\n",
    "    df.drop(columns=['absence_category_list'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
